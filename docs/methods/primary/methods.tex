\documentclass{article}

\usepackage{natbib}

% for equation*
\usepackage{amsmath}

% for scalebox,...
\usepackage{graphics}

% hide hyperref links  with pdfborder (more portable than hidelinks option)
\usepackage[pdfborder={0 0 0}]{hyperref}

% for pseudocode
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\title{'Methods for Strelka Small Variant Caller'}


% simple scientific notation:
\newcommand{\e}[1]{\ensuremath{\times 10^{#1}}}

\begin{document}

\maketitle

\tableofcontents



\section{Purpose}

On any release branch, the methods described here should reflect the default implementation in the source repository containing this document.



\section{Workflow Overview}

Strelka comprises several workflows to call small variants from mapped sequencing data. Supported small variant calling workflows include detection of germline variants in a set of samples, and somatic variants from matched tumor-normal sample pairs.

The methods below describe several core components shared by all application workflows, in addition to model customizations made for each type of variant calling problem. All workflows share a common sequence of analysis steps:

\begin{enumerate}
\item Preliminary estimation of parameters from sample data.
\item Division of the genome into segments to be analyzed in parallel
\item For each analyzed genome segment:
\begin{enumerate}
\item Filtration of input read alignments
\item Processing the input read alignments into a set of variant haplotype candidates.
\item Finding probability of variant haplotypes/haplotype combinations under various (germline, somatic, de-novo) models.
\item Empirical re-scoring of variants based on models trained from static truth sets.
\end{enumerate}
\item Joining parallel analysis results into final reported output format.
\end{enumerate}

The details of the shared and application specific model steps are provided below.



\section{Preliminary Parameter Estimation}

Various aggregate statistics may be estimated from the sequencing data prior to the start of the primary variant calling procedure.

\subsection{Chromosome depth estimation}
\label{sec:depth_est}

An initial step in all workflows is the rapid estimation of the sequencing depth for each chromosome. For somatic analysis this depth is only computed for the normal sample. This information is used downstream to filter out high-depth regions (details below) when Strelka is run in its default whole genome sequencing mode. This step is skipped for exome or other targeted analyses.

For each chromosome, depth is estimated using a modified median calculation. As a first step, each chromosome is partitioned into segments of similar size to ensure the estimation process samples several chromosome regions. The chromosome is divided into the smallest number of segments no larger than $S_{max}$, where all segments have nearly equal size (all sizes must be $S$ or $S+1$, given smallest segment size of $S$). If this procedure results in more than 20 segments then $S_{max}$ is doubled until 20 or fewer segments are found. $S_{max}$ is initialized to 2 Mbase.

The depth estimation procedure repeatedly cycles through all segments. Within each segment, at least 40,000 reads are scanned before moving to the next segment. Whenever a read is scanned at a given mapping position, additional reads are scanned until the mapping position changes. After the target number of reads have been scanned from every segment in the chromosome, the procedure returns to the first position and repeats this procedure starting from the last unscanned position. The process repeats until the all reads in all segments are scanned or the depth estimate converges.

Each scanned read is filtered if it is unmapped. Otherwise the read alignment is ignored and the read is applied to a simple depth pileup assuming a complete and ungapped alignment starting from the mapping position. Every 1M reads triggers a convergence check, but only after every chromosome segment has been sampled at least once.

Depth is estimated from the resulting pileup, using a simple median over all depth observations from the pileup structure excluding zero counts. Convergence (defined as an absolute change of less than 0.05 or, in the median case, as an exact match) is checked between the depth estimate of the last convergence check and the current one.

The depth estimation procedure is run separately for each non-tumor sample, and all high-depth thresholds are set based on the sum of depth estimates over these samples.


\subsubsection{Depth estimation design discussion}

The objective of the depth estimation step is to find the expected depth and/or depth distribution for each copy number state or plasmid -- at present dividing this estimation to run separately on each chromosome is a reasonable approximation towards this goal. An improvement would be to estimate the depth distribution properties jointly over each copy number state and plasmid.


\subsection{Indel Error Rate Estimation}

TODO: Transfer/adapt methods already written up in the errorAnalysis doc

\paragraph{Implementation}

Indel error estimation is only run on chromosomes which are at least 5 Mb in length. If the combined length of all such chromosomes is less than 50 Mb, indel error estimation is disabled and the method reverts to static parameters for all samples.

If the reference sequence is determined to be acceptable for the estimation process, then the set and ordering of genome regions used to count errors is determined. This is done by finding the subset of chromosomes which are 5 Mb or longer, and dividing each of these chromosomes into the smallest number of approximately equal length segments which are not larger than 2 Mbs each. These intervals are randomly sorted using a fixed seed to help distribute the estimation process over all large chromosomes while providing a deterministic response over multiple runs.

After determining the set and order of estimation intervals, error counting and parameter estimation proceeds independently for each sample. In each sample, counting tasks are launched in order from the estimation interval list up to the number of available workers in the workflow. As each counting procedure completes, the number of sites with non-zero sequence coverage is summed into a running total from all completed tasks. Additional tasks from the estimation interval list are launched until the total coverage of non-empty sites from completed tasks is greater than or equal to 50 Mb. When this site coverage goal is readhed, then tasks which are no longer needed are canceled, and the workflow waits for tasks to complete so as to cover the smallest continuous series of estimation intervals from the beginning of the estimation interval list which provide coverage of 50 Mb or greater. The workflow waits for this continuous series of tasks so that the final set of counts is deterministic regardless of the completion order of the launched estimation tasks.

Once the counting procedure has completed for a given sample, all of the estimation interval count files are merged, and parameter estimation is conducted according to the procedure outlined above. If parameter estimation fails to converge after the maximum allowed iterations or the estimated error rates fall outside of valid estimation ranges, then the estimation procedure is considered a failure for the given sample, and the default static parameter set is used for that sample instead.


\section{Variant Discovery}


\subsection{Input Read Filtration}

TODO. Outright filtration of unmapped, anomalous, etc. Note additional category of reads that are marked for special processing but not actually filtered (low MAPQ, etc.)


\subsection{Input Read Processing}

TODO. Left-shifting and other normalizing transformations of all indels present in the reads input alignments.


\subsection{Short Haplotyping}

Alignments in polymorphic regions are prone to errors, often leading to false variant calls. To cope with this problem, state-of-the-art germline variant callers like GATK and Platypus use local de novo assembly to reconstruct haplotypes in these regions and use them to improve variant calls. We employ a similar haplotyping approach called {\em short haplotyping}. For runtime efficiency, the haplotyping approach is applied only to relatively short regions ($\leq 250$ by default) where dense variants are expected. Also, currently it is enabled only in germline variant calling.

The short haplotyping is comprised of 3 steps: (1) detecting short regions with dense variants called {\em active regions}, (2) creating candidate haplotypes, and (3) aligning each haplotype to the reference to discover primitive alleles such as SNVs and short indels. In multi-sample settings, short haplotyping is conducted independently for each sample.

\subsubsection{Active region detection}
To detect active regions, we first identify loci that are likely to have variants, which we call {\em variant loci}. To identify variant loci, for each locus we calculate an alignment depth (the number of alignments overlapping the locus) and a variant count (weighted count of alignments with variants at the locus). Variant counts are updated while reading alignments. A mismatch of an alignment at locus $i$ increases the counter at locus $i$ by 1. An insertion of a sequence between locus $i$ and $i+1$ increases the counters at $i$ and $i+1$ by 4. A deletion of loci $[i,j]$ increases the counters in $[i-1,j]$ by 4. Similar to an indel, a soft-clipped segment also increases the counter. A beginning (or ending) edge soft-clipped segment ending (or starting) at locus $i$ increases the counter at $i$ and $i+1$ (or $i-1$) by 4. After all alignments overlapping a locus are read, a decision is made whether the locus is a variant locus. A locus with a variant count $c$ and an alignment depth $d$ becomes a variant locus if (1) $c \geq 0.35 \cdot d$ or (2) $c \geq 9$ and $c \geq 0.2 \cdot d$.

After variant loci have been detected, nearby variant loci are clustered if they are within 13 bases of each other. For the clusters including two or more variant loci, the cluster region is further extended to the surrounding loci so that the first and last locus are not within a homopolymer or short tandem repeat (STR) region. This extension is needed because alignments that do not fully span such repeats are often erroneous and relying on them may lead to generating incorrect haplotypes. To accomplish this, we detect {\em anchor loci} that are not variant loci and also do not belong to a homopolymer (of lengths no less than 3) or STR (of repeat unit lengths between 2 and 50). Given a cluster of variant loci, the active region is created between the closest anchor loci before and after the first and last variant loci.

Note that the active region detection method is designed so that the vast majority of indels are included in active regions. Even for a single base deletion at locus $i$, each alignment having the deletion increases the counters at $i-1$ and $i$ by 4, thus only three of such alignments trigger creation of an active region including $i-1$ and $i$.


\subsubsection{Haplotype generation}
Given an active region of size 250 or smaller, an attempt is made to generate haplotypes using either simple counting or local de novo assembly. The decision whether to run counting or assembly is taken by considering the fraction of reads that fully cover the active region (called covering reads). Out of all reads that overlap with an active region, if 65\% or larger reads are covering reads, counting is run. The counting procedure is simple. From each covering read, the segment aligned to the region is extracted. If a segment $s$ is extracted from a read $r$, we call $r$ a supporting read of $s$. For each unique read segment, the number of supporting reads is counted. Afterwards, up to two candidate haplotypes are selected by picking the read segments with the largest supporting read counts, with the restriction that the counts have to be 3 or larger.

If the fraction of covering reads is less than 65\%, local de novo assembly is run using a de-Bruijn graph approached detailed in section \ref{sec:HaplotypeAssembler}. The chance that the assembler returns true contigs increases when the contigs do not start or end with repeats. To take advantage of this, given an active region $[i,j]$, we extend it up to 9 bases upstream and downstream until it encounters a variant locus. Thus the extended active region $[i',j']$ satisfies the following conditions: $i-9 \leq i' \leq i$, $j \leq j' \leq j+9$, and there is no variant locus in $i' \leq pos \leq i-1$ and $j+1 \leq pos \leq j'$. Since $i$ and $j$ are anchor loci, this extension makes the correct contigs returned by the assembler to share the same prefix (reference segment at $[i',i]$; denoted by a prefix anchor) and suffix (reference segment at $[j,j']$; denoted by a suffix anchor).

All the reads that (fully or partially) overlap with the extended active region are used in the assembly. For the sake of speed, if the number of reads exceed 1000, assembly is not performed. After assembly is finished, from all the contigs returned by the assembler, only the contigs including both prefix and suffix anchors are selected, and from them up to two contigs with the largest supporting read counts are chosen, again with the restriction that the counts should be 3 or larger. From these top two contigs, the prefix and suffix anchors are detached, and the resulting sequences become the candidate haplotypes.

{\tt TODO: description of sequencer phasing filter}

If the candidate haplotypes are generated by counting, or at least one candidate haplotype other than the reference is generated by assembly, the haplotyping is regarded as successful.

\subsubsection{Haplotype assembly}
\label{sec:HaplotypeAssembler}
{\tt TODO: description of iterative assembler}

\subsubsection{Primitive allele discovery}
If haplotyping is successful, the candidate haplotypes are aligned to the reference using a global aligner that employs an affine gap penalty function with the following parameters: match score 1, mismatch penalty 4, gap opening penalty 5, and gap extension penalty 1. From the alignment, primitive alleles (SNVs and indels of size 50 and smaller) are identified and marked {\em discovered} in an active region. These discovered primitive alleles are used to improve SNV and indel calling by (1) relaxing the mismatch density filter (described in \ref{sec:BasecallFiltrationFromContext}), (2) improving indel candidacy (described in \ref{sec:IndelCandidacy}), and (3) phasing SNVs and indels within the same active region (described in \ref{sec:ReadBackedPhasing}).

\subsection{Basecall filtration from local read context}
\label{sec:BasecallFiltrationFromContext}

{\tt TODO: description of the mismatch density filter (MMDF)}
%Strelka filters out fractions of reads, where the densities of mismatches and indels are high.

If short haplotyping is enabled, SNVs discovered in active regions are used to relax the MMDF. For this, the active region detector records all the SNVs discovered in active regions separately for each sample. When the mismatch density is calculated for a read, any mismatch at position matching a candidate SNV is exempted from measuring the mismatch density.


\subsection{Indel Candidacy}
\label{sec:IndelCandidacy}

Strelka uses indel candidacy as a preliminary filter to eliminate indel observations from consideration as variants if they are very likely to have been generated by error processes.  A candidate indel variant must minimally have 2 reads supporting it in at least one sample (this is relevant in multi-sample settings, such as somatic, trio de novo, and germline multi-sample calling). If short haplotyping is enabled, a candidate indel belonging to an active region where haplotyping was successful must also have been discovered through haplotype alignment in at least one sample. If an indel observation satisfies these conditions, Strelka evaluates its candidacy status using a one-sided binomial exact test, with the null hypothesis being that the indel coverage is generated by indel error processes.

Given a total locus coverage of $N$, indel coverage of $n_i$, and an indel error rate of $e_l$ (described below in ``\nameref{sec:indel_error}''), we define the probability of some coverage $x$ being generated at the locus due to indel error as
\begin{equation*}
P(x \mid N, e_l) = \binom {N} {x} e^{x}_l (1 - e_l)^{N - x}
\end{equation*}
We can then define the probability of generating an indel with at least coverage $n_i$ due to indel error at a locus as follows:
\begin{equation*}
p_{error} = P(X >= n_i \mid N, e_l) = \sum_{x = n_i}^{N} P(x \mid N, e_l)
\end{equation*}

\noindent The indel is considered as a candidate variant if $p_{error} < p_{reject}$, where $p_{reject}$ is the rejection threshold for the null hypothesis. The default value $p_{reject}$ is $1\e{-9}$.


\subsection{Read Realignment}

\subsubsection{Handling of soft-clipped segments during realignment}

By default, prior to a read being realigned, any soft-clipped segment on the leading and trailing edge is unrolled to force the segment to be aligned to the reference. This treatment assumes that most soft-clipping reflects an indel which has not yet been incorporated into the read's alignment.

During RNA-Seq analysis, a similar soft-clipping approach is taken, except that at the end of the realignment process, the highest scoring alignment found after unrolling all soft-clipping is compared to the original alignment. If the original alignment has a higher score, then the realignment procedure reverts to the original.



\section{Variant Probability Model}


\subsection{Shared}

This section describes components of the probability model shared by all calling modes.

\subsubsection{Indel Error Model}
\label{sec:indel_error}
Indel errors are approximated as a process which occurs independently in each read, with some fixed probably of an indel error occurring as a function of the local sequence STR context. Strelka's indel error parameters are currently set to pre-estimated values based on empirical observation of indel calling performance for both germline and somatic indel detection.

The default indel error rates used by strelka are a function of the local homopolymer context length $l_{\text{STR}}$, so any expansion or contraction of a homopolymer sequence has an error rate indexed on the homopolymer length in the presumed source haplotype upon which the indel error process occurred. All other types of indels, including dinucleotide tract indels and mutations of homopolymers which do not represent simple expansion/contraction events, have $l_{\text{STR}}$ set to one. By observation, the indel error rates are set to a log-linear ramp as a function of $l_{\text{STR}}$

\begin{equation*}
P(error \mid l_{\text{STR}}) = e_{l} \exp(f_{\text{STR}}(\log(e_{h})-\log(e_{l})))
\end{equation*}

\noindent where the constant indel errors set for low and high STR lengths are $e_{l} = 5\e{-5}$ and $e_{h} = 3\e{-4}$, and the fraction of high STR length is $f_{\text{STR}} = \max((l_{\text{STR}}-1),15)/15$.

As a special case, the germline indel genotyping model uses higher error rates to heuristically account for assembly and mapping errors. This is currently a scaling factor of 100, $P(error_{\text{germline}} \mid l_{\text{STR}}) = 100 * P(error \mid l_{\text{STR}})$


\subsection{Germline Model}


TODO

% snv strand bias note:
% dgt.strand_bias=std::max(lhood_fwd[tgt],lhood_rev[tgt])-lhood[tgt];

\subsubsection{Germline Variant Phasing}
\label{sec:ReadBackedPhasing}
As previously noted, Strelka defines an active region around dense variants and infers 2 haplotypes for the region. These haplotypes are used to phase SNVs and indels within the same active region. The phasing is conducted after scoring and genotyping. For each heterozygous variant belonging to an active region, Strelka checks which haplotype the variant is lying on. If the variant is lying on either of the two haplotypes, it is marked phased and the haplotype ID (0 or 1) is recorded to appropriately write the genotype allele order (e.g. 0|1 or 1|0).


\subsection{Somatic Model}

TODO: update quality score names in this section

The somatic calling model assumes that the samples are from diploid individuals. For both SNVs and indels, the normal genotype state space is the set of unphased diploid genotypes, $G_n = \{ \texttt{ref}, \texttt{het}, \texttt{hom}\}$, where $\texttt{ref}$, $\texttt{het}$, and $\texttt{hom}$ refer to the normal genotype being reference, heterozygous, and homozygous, respectively. The tumor genotype has two states $G_t = \{ \texttt{nonsom}, \texttt{som} \}$, where $\texttt{nonsom}$ and $\texttt{som}$ indicate the absence and presence of somatic mutation in the tumor sample, respectively. The method approximates a posterior probability on the joint tumor and normal genotypes.

\begin{align*}
	& P(G_t,G_n \mid D) \propto P(G_t,G_n) P(D \mid G_t,G_n) \\
\end{align*}


Here $D$ refers to the sequencing data from both samples. The likelihood term above is computed from independent sample-specific genotype likelihoods

\begin{align*}
	& P(D \mid G_t,G_n) = \int_{F_t,F_n}{P(D \mid F_t,F_n)P(F_t,F_n \mid G_t,G_n)} \\
	& = \int_{F_t,F_n}{P(D_t \mid F_t)P(D_n \mid F_n)P(F_t,F_n \mid G_t,G_n)},
\end{align*}

\noindent where $F_t$, $F_n$ refer to tumor and normal allele frequencies and $D_t$ and $D_n$ indicate tumor and normal sample data. In the above equation, the sample-specific allele frequency likelihoods $P(D_t \mid F_t)$ and $P(D_n \mid F_n)$ could in principle be supplied by any probabilistic variant caller. Strelka currently provides its own sample-specific frequency likelihoods (described in Section xxx). The genotype prior probability $P(G_t, G_n)$ and the joint allele-frequency distribution $P(F_t,F_n \mid G_t,G_n)$ are detailed below.

The posterior probability over tumor and normal genotypes $P(G_t,G_n \mid D)$ is used to compute the {\em somatic variant probability} (QSS for SNVs and QSI for indels).

\begin{equation}
\label{eqn:somVarProb}
	P(G_t = \texttt{som} \mid D) = \sum_{G_n \in \{ \texttt{ref}, \texttt{het}, \texttt{hom} \}}{P(G_t=\texttt{som},G_n \mid D)}.
\end{equation}

As previously discussed, this somatic variant probability is not ideal for detecting variants of interest because it does not distinguish somatic variant types. We therefore associate somatic calls with the probability of somatic variation {\em and} the normal sample genotype being $\texttt{ref}$ (QSS\_NT for SNVs and QSI\_NT for indels), i.e., $P(G_t = \texttt{som}, G_n = \texttt{ref} \mid D)$. All Strelka quality scores discussed below express these values.


\subsubsection{Tiered observations}
\label{sec:somatic_tiers}

TODO

The Strelka workflow uses two calling tiers. All somatic calls are classified according to their most-likely normal genotype if that value is the same in tiers 1 and 2, and classified as conflicts otherwise.

\subsubsection{Genomic prior}
Given the expected rate of variants between two unrelated haplotypes (Watterson theta) $\theta$, the normal genomic prior $P(G_n)$ over the set of diploid genotypes is

\begin{equation*}
P(G_n)=
\begin{cases}
	\theta & \text{if } G_n = \texttt{het} \\
	\theta/2 & \text{if } G_n = \texttt{hom} \\
	1 - 3\theta/2 & \text{if } G_n = \texttt{ref} \\
\end{cases}
\end{equation*}

\noindent where $\theta_{\text{SNV}}=1\e{-3}$ for SNVs and $\theta_{\text{indel}}=1\e{-4}$ for indels. Given the somatic state prior $P(G_t=\texttt{som}) = \gamma$, the genomic prior is

\begin{equation*}
P(G_t, G_n)=
\begin{cases}
	(1 - \gamma) P(G_n) & \text{if } G_t = \texttt{nonsom} \\
	\gamma P(G_n) & \text{if } G_t = \texttt{som} \\
\end{cases}
\end{equation*}

\noindent where $\gamma$ is set to $\gamma_{\text{SNV}} = 1\e{-3}$ and $\gamma_{\text{indel}} = 1\e{-6}$ for SNVs and indels. Note that $\gamma$ is expected to scale the somatic variant probabilities but not substantially influence their rank, thus its value was chosen empirically to provide reasonable variant probabilities and are not adjusted for different samples in practice.


\subsubsection{Joint allele-frequency prior}
The prior probability on the tumor and normal allele-frequencies $P(F_t, F_n \mid G_t, G_n)$ encodes the concept that the normal sample is a mixture of diploid germline variation and noise while the tumor sample is a mixture of the normal sample and somatic variation. Let $\mathcal{C} (F_n, G_n) = 1$ if $F_n$ is a {\em canonical} diploid allele frequency of $G_n$ and $\mathcal{C} (F_n, G_n) = 0$ otherwise. For example, $\mathcal{C} (F_n=0, G_n = \texttt{ref}) = 1$ and $\mathcal{C} (F_n=0.4, G_n = \texttt{ref}) = 0$. The joint frequency prior is defined as follows.

% Non-somatic
\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{nonsom}, G_n)=
\begin{cases}
	0 & \text{ if } F_t \neq F_n \\
	1-\mu & \text{ if } F_t = F_n \text{ and }\mathcal{C}(F_n, G_n) = 1 \\
	\mu U(F_t) & \text{ if } F_t = F_n \text{ and }\mathcal{C}(F_n, G_n) = 0 \\
\end{cases}
\end{equation*}

% Somatic, normal genotype ref
\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{som}, G_n = \texttt{ref})=
\begin{cases}
	(1-\mu)U(F_t) & \text{ if } F_t \neq F_n \text{ and } F_n \leq \tau F_t \\
	0 & \text{ otherwise } \\
\end{cases}
\end{equation*}

% Somatic, normal genotype het or hom
\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{som}, G_n \neq \texttt{ref})=
\begin{cases}
	(1-\mu)U(F_t) & \text{ if } F_t \neq F_n \text{ and } \mathcal{C}(F_n, G_n) = 1 \\
	0 & \text{ otherwise } \\
\end{cases}
\end{equation*}

\noindent Here, $\tau$ represents the {\em contamination tolerance}, $U(F_t)$ refers to a uniform distribution over the allowed tumor allele frequencies and $\mu$ indicates the noise term. The contamination tolerance term is introduced to allow for contamination in the normal sample by some fraction of tumor cells. This is particularly useful for analyses of liquid tumors, where normal sample is almost always contaminated by tumor cells. The contamination tolerance is set to $0.15$. The noise term abstracts various sequencing, read mapping and assembly issues which could produce an unexpected allele frequency shared in the tumor and normal samples. For SNVs, the noise contribution is set to $\mu_{\text{SNV}} = 5\e{-10}$. For indels, it is dynamically estimated as described in Section xxx.

% The description of the strand-bias model is not included here because it is not used in the quality score calculation.

\subsubsection{Practical computation}

The continuous allele frequencies modeled above are efficiently computed by dividing each allele-pair axis into a set of equidistant points and performing the somatic probability computation over the resulting discrete point set. Several point resolutions were attempted to confirm the expected stability and convergence of results as resolution increased. A resolution of 21 points per axis is used for all computations by default. We expect that this resolution should be increased for improved detection of somatic allele frequencies lower than 5\%.

\subsubsection{Somatic callability track}

The somatic variant calling workflow optionally provides a somatic callability track expressing, for each reference position, whether there is sufficient sequencing evidence to confidently assert either (1) the presence of a somatic SNV or (2) the absence of a somatic SNV with somatic variant frequency of 10\% or higher. Evidence for the presence of a somatic SNV is quantified by the QSS score described above in equation \ref{eqn:somVarProb}. Evidence for the absence of a somatic SNV is quantified by the \emph{non-somatic SNV probability} (or the equivalent phred-scaled score, NQSS) described below. A reference position is marked as "callable" in the somatic callability track if QSS $\ge$ 15 or NQSS $\ge$ 15.

The NQSS score is computed from the same sample-specific allele frequency likelihoods, $P(D_t \mid F_t)$ and $P(D_n \mid F_n)$, used to compute QSS; however, the genotype prior probability $P(G_t, G_n)$ and the joint allele-frequency distributions $P(F_t,F_n \mid G_t,G_n)$ are adjusted as described below.

The diploid genotype prior used for the NQSS score is

\begin{equation*}
P(G_n\mid \text{NQSS})=
\begin{cases}
0 & \text{if } G_n = \texttt{het} \\
1/2 & \text{if } G_n = \texttt{hom} \\
1/2 & \text{if } G_n = \texttt{ref} \\
\end{cases}
\end{equation*}

The prior distribution over somatic genotypes is uniform, thus the full genotype prior used for the NQSS score is

\begin{equation*}
P(G_t, G_n \mid \text{NQSS}) =
\begin{cases}
P(G_n \mid \text{NQSS})/2 & \text{if } G_t = \texttt{nonsom} \\
P(G_n \mid \text{NQSS})/2 & \text{if } G_t = \texttt{som}. \\
\end{cases}
\end{equation*}


The joint allele-frequency distribution used for the NQSS score is

\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{nonsom}, G_n, \text{NQSS})=
\begin{cases}
U(F_t) & \text{ if } F_t = F_n \\
0 & \text{ otherwise } \\
\end{cases}
\end{equation*}

\begin{equation*}
P(F_t, F_n \mid G_t = \texttt{som}, G_n, \text{NQSS})=
\begin{cases}
U(F_t)/2 & \text{ if } F_t \neq F_n, G_n \in \{\texttt{ref},\texttt{hom}\}  \\
0 & \text{ otherwise } \\
\end{cases}
\end{equation*}

Note that the somatic callability track is based on the core somatic variant quality model and does not reflect additional information about each candidate variant integrated into the empirical variant scoring step described below. For this reason the somatic calling track will diverge from Strelka's final somatic SNV output in some cases.



\section{Filtration and Empirical Scoring}

The variant calling models (both germline and somatic) provide useful representations of the sequencing data at putative variant loci; however, there is additional information not used by the models which is predictive of call accuracy. These may be metrics such as the number of reads or alleles which are filtered out of the call quality computation, the quality of alignments in the region around the putative variant or various allele/strand/quality biases indicative of assay or mapping artifacts. As a final step in the variant calling process, such additional information is enumerated as a set of predictive features and used to improve call precision for a given recall level beyond what can be achieved from the core variant calling model alone. These additional features are employed in the final step of variant calling in one of two ways:

\begin{itemize}
    \item Empirical Variant Scoring (EVS):
    The EVS model is used to provide a single aggregate quality score for each variant which incorporates the information from all variant calling features. This model is used to improve variant call precision for a given recall level as stated above, but also provides additional benefits: (1) the EVS model tends to provide a greater precision improvement than simple hard-filtration of the features (see below) (2) consolidating all accuracy predictors to a single metric allows for an optimized exploration of the full ROC curve for applications which require much higher recall or precision than provided with default variant passing thresholds (3) the training mechanism provides a pathway to create calibrated quality values.
    \item Hard Filters:
    When the EVS model is not used, simple cutoffs are applied to a set of features (not necessarily the same set used by EVS) to filter out calls which are not sufficiently likely to be real. Hard filters are used at all homozygous reference sites, for any contig where the continuous (heteroplasmic) calling option has been selected, and for any assay where EVS is turned off by default (such as enrichment and amplicon-based targeted sequencing).
\end{itemize}


\subsection{EVS model}

Empirical variant scoring in Strelka uses pre-trained random forest models taking EVS features as input to produce the probability of an erroneous variant call. For each of germline, RNA-seq, and somatic variant calling, there are two separate trained random forest models and feature sets for the two high-level variant categories, SNVs and indels. Strelka is intended to run with models which have been pre-trained on a combined training data set representing a wide variety of sample-prep and sequencing assays. Note that although scripts are provided to recreate the EVS model training procedure, there is no intention for the models to be retrained for each input sequencing dataset to be analyzed (this is in contrast to dynamic re-scoring systems such as the GATK VQSR procedure \cite{depristo2011}).

The EVS models are trained using the random forest learning procedures implemented in the scikit-learn package \cite{scikit-learn}, trained on a set of candidate calls with truth labels assigned as described below. For the germline and RNA-seq EVS models, each random forest uses 50 decision trees with a maximum depth of 12, a minimum of 50 samples per leaf, and no limit on the maximum number of features. For the somatic EVS models, each random forest uses 100 decision trees with a maximum depth of 6. The remaining options are set to scikit-learn defaults.

The training data are compiled from a collection of sequencing runs using different sample prep, sequencing platforms and chemistries. All germline and RNA-seq datasets are from an individual for which a gold standard truth set is available from the Platinum Genomes project \cite{eberle2017}. Candidate variants that correspond to the high-confidence regions of the truth set are labeled as true or false using the hap.py haplotype comparison tool (\urlstyle{same}\url{https://github.com/Illumina/hap.py}). Variants that exist in the truth set but were called with incorrect genotype are treated as false variants, with the exception of the RNA-seq model which treats them as true variants. In the case of germline calling, it is believed that the vast majority of candidate SNVs (but not indels) outside of the high-confidence regions (and classified by hap.py as unknown) are false; for this reason, these variants are added to the set of false variants that are presented to the model during training, downweighted so as to have a total weight which is half the total weight of the known false variants.

Somatic datasets include simulated tumor-normal pairs from the Platinum Genomes project as well as tumor-normal data from real tumor cell lines for which curated (but generally noisier) truth sets have been constructed. Labeling of somatic datasets is done by means of a script included in the Strelka distribution (see user guide).

The output scores produced by the random forest classifier are transformed to phred-scale and calibrated by passing the resulting quality values through a linear transform estimated by regressing binned empirical quality onto predicted quality. Finally, variant filter labels are assigned based on thresholds that have been selected to achieve a reasonable tradeoff across multiple datasets.

\paragraph{Hard-filter applied when the read depth is low}
For germline site loci, read depths are calculated from base calls used for site genotyping. For germline indel loci, read depths are taken from the read depths of the sites preceding the indels. For all loci with read depths below 3, (variant or homozygous reference) genotype calls are supplemented with a LowDepth filter. For variant loci, allelic depths for the reference and alternative alleles are additionally considered. If the sum of the allelec depths is below 3, variant calls are supplemented with a LowDepth filter.

For somatic variants, tumor sample read depths are calculated for SNVs and indels in a similar way. If read depths are below 2, somatic variant calls are supplemented with a LowDepth filter.

\paragraph{Hard-filters applied when the EVS model is used}

When the EVS model is used, most variants are filtered based on the score computed by the random forest model. In some cases this score is supplemented with additional hard filters.

For somatic indels, the following filter is added:

\begin{itemize}
    \item \texttt{NormalSampleRelativeTotalLocusDepth} ~ variant is filtered if this value is greater than 3
\end{itemize}


\paragraph{Features used by the EVS model}

Features used in each model are listed here and definitions are provided further below.

\begin{itemize}
    \item Germline SNV features:
    \begin{itemize}
        \item \texttt{GenotypeCategory}
        \item \texttt{SampleRMSMappingQuality}
        \item \texttt{SiteHomopolymerLength}
        \item \texttt{SampleStrandBias}
        \item \texttt{SampleRMSMappingQualityRankSum}
        \item \texttt{SampleReadPosRankSum}
        \item \texttt{RelativeTotalLocusDepth}
        \item \texttt{SampleUsedDepthFraction}
        \item \texttt{ConservativeGenotypeQuality}
        \item \texttt{NormalizedAltHaplotypeCountRatio}
    \end{itemize}
    \item Germline Indel features:
    \begin{itemize}
        \item \texttt{GenotypeCategory}
        \item \texttt{SampleIndelRepeatCount}
        \item \texttt{SampleIndelRepeatUnitSize}
        \item \texttt{SampleIndelAlleleBiasLower}
        \item \texttt{SampleIndelAlleleBias}
        \item \texttt{SampleProxyRMSMappingQuality}
        \item \texttt{RelativeTotalLocusDepth}
        \item \texttt{SamplePrimaryAltAlleleDepthFraction}
        \item \texttt{ConservativeGenotypeQuality}
        \item \texttt{InterruptedHomopolymerLength}
        \item \texttt{ContextCompressability}
        \item \texttt{IndelCategory}
        \item \texttt{NormalizedAltHaplotypeCountRatio}
    \end{itemize}

    \item RNA-seq SNV features:
    \begin{itemize}
        \item \texttt{SiteHomopolymerLength}
        \item \texttt{SampleStrandBias}
        \item \texttt{SamplePrimaryAltAlleleDepth}
        \item \texttt{VariantAlleleQuality}
        \item \texttt{SampleMeanDistanceFromReadEdge}
        \item \texttt{SamplePrimaryAltAlleleDepthFraction}
    \end{itemize}
    \item RNA-seq Indel features:
    \begin{itemize}
        \item \texttt{SampleRefAlleleDepth}
        \item \texttt{SamplePrimaryAltAlleleDepth}
        \item \texttt{SampleIndelRepeatCount}
        \item \texttt{SampleIndelRepeatUnitSize}
        \item \texttt{VariantAlleleQuality}
        \item \texttt{SampleIndelMeanDistanceFromReadEdge}
        \item \texttt{SampleRefRepeatCount}
        \item \texttt{SamplePrimaryAltAlleleDepthFraction}
    \end{itemize}

    \item Somatic SNV features:
    \begin{itemize}
        \item \texttt{SomaticSNVQualityGivenGermlineGenotype}
        \item \texttt{NormalSampleRelativeTotalLocusDepth}
        \item \texttt{TumorSampleAltAlleleFraction}
        \item \texttt{RMSMappingQuality}
        \item \texttt{ZeroMappingQualityFraction}
        \item \texttt{TumorSampleStrandBias}
        \item \texttt{TumorSampleReadPosRankSum}
        \item \texttt{AlleleCountLogOddsRatio}
        \item \texttt{NormalSampleFilteredDepthFraction}
        \item \texttt{TumorSampleFilteredDepthFraction}
    \end{itemize}
    \item Somatic Indel features:
    \begin{itemize}
        \item \texttt{SomaticIndelQualityGivenGermlineGenotype}
        \item \texttt{TumorSampleReadPosRankSum}
        \item \texttt{TumorSampleLogSymmetricStrandOddsRatio}
        \item \texttt{RepeatUnitLength}
        \item \texttt{IndelRepeatCount}
        \item \texttt{RefRepeatCount}
        \item \texttt{InterruptedHomopolymerLength}
        \item \texttt{TumorSampleIndelNoiseLogOdds}
        \item \texttt{TumorNormalIndelAlleleLogOdds}
        \item \texttt{AlleleCountLogOddsRatio}
    \end{itemize}
\end{itemize}

\subsection{Hard filters}

In this case a set of filters is defined. Each filter is triggered when a single feature exceeds some critical value. Filtration thresholds are set to remove variants which are very likely to be incorrect.

The hard filter model is used whenever EVS cannot be, in particular this is the method used to filter germline homozygous reference calls. The EVS model may be turned off for all variants whenever the assay conditions are suspected to poorly match the EVS models training conditions, in which case the hard filters will be used instead.

\paragraph{Hard filter thresholds for germline model}

\begin{itemize}
    \item Shared filter conditions
    \begin{itemize}
        \item \texttt{ConservativeGenotypeQuality} ~ Variant is filtered if this value is less than 15
        \item \texttt{RelativeTotalLocusDepth} ~ Variant is filtered if this value is greater than 3
    \end{itemize}
    \item SNV-specific filter conditions:
    \begin{itemize}
        \item \texttt{SampleStrandBias} ~ SNV is filtered if this value is greater than 10
    \end{itemize}
\end{itemize}

\paragraph{Hard filter thresholds for somatic model}

\begin{itemize}
    \item Shared filter conditions
    \begin{itemize}
        \item \texttt{NormalSampleRelativeTotalLocusDepth} ~ Variant is filtered if this value is greater than 3
    \end{itemize}
    \item SNV-specific filter conditions:
    \begin{itemize}
        \item \texttt{SomaticSNVQualityGivenGermlineGenotype} ~ SNV is filtered if this value is less than 15 or if the most likely normal sample genotype at the site is not homozygous reference
        \item \texttt{SiteFilteredBasecallFrac} ~ SNV is filtered if this value is greater than or equal to 0.4
        \item \texttt{SpanningDeletionFraction} ~ SNV is filtered if this value is greater than 0.75
    \end{itemize}
    \item Indel-specific filter conditions:
    \begin{itemize}
        \item \texttt{SomaticIndelQualityGivenGermlineGenotype} ~ Indel is filtered if this value is less than 40 or if the most likely normal sample genotype for the somatic allele is not homozygous reference
        \item \texttt{IndelWindowFilteredBasecallFrac} ~ Indel is filtered if this value is greater than or equal to 0.3
    \end{itemize}
\end{itemize}


\subsection{EVS/hard-filter feature descriptions}

\subsubsection{Shared features}
\begin{itemize}

    \item \texttt{InterruptedHomopolymerLength} ~ One less than the length of the longest interrupted homopolymer in the reference sequence containing the current position. An interrupted homopolymer is a string that has edit distance 1 to a homopolymer.

\end{itemize}


\subsubsection{Germline and RNA-seq features}
\begin{itemize}

    \item \texttt{GenotypeCategory} ~ A category variable reflecting the most likely genotype as heterozygous (0), homozygous (1) or alt-heterozygous (2).

    \item \texttt{SampleRMSMappingQuality} ~ RMS mapping quality of all reads spanning the variant in one sample. This feature matches SAMPLE/MQ in the VCF spec.

    \item \texttt{SiteHomopolymerLength} ~ Length of the longest homopolymer containing the current position if this position can be treated as any base.

    % TODO note this metric doesn't allow for any strand bias, which could be problematic at high depth. High values don't indicate high bias, but high confidence in any level of bias, no matter how small. This may not be exactly what we want: this feature might have an optimal threshold which changes as a function of depth:
    \item \texttt{SampleStrandBias} ~ Log ratio of the sample's genotype likelihood computed assuming the alternate allele occurs on only one strand vs both strands (thus positive values indicate bias).

    \item \texttt{SampleRMSMappingQualityRankSum} ~ Z-score of Mann-Whitney U test for reference vs alternate allele mapping quality values in one sample.

    \item \texttt{SampleReadPosRankSum} ~ Z-score of Mann-Whitney U test for reference vs alternate allele read positions in one sample.

    \item \texttt{RelativeTotalLocusDepth} ~ Locus depth relative to expectation: this is the ratio of total read depth at the variant locus in all samples over the total expected depth in all samples. Depth at the variant locus includes reads at any mapping quality. Expected depth is taken from the preliminary depth estimation step defined above in ``\nameref{sec:depth_est}''. This value is set to 1 in exome and targeted analyses, because it is problematic to define expected depth in this case.

    \item \texttt{SampleUsedDepthFraction} ~ The ratio of reads used to genotype the locus over the total number of reads at the variant locus in one sample. Reads are not used if the mapping quality is less than the minimum threshold, if the local read alignment fails the mismatch density filter or if the basecall is ambiguous.

    \item \texttt{ConservativeGenotypeQuality} ~ The model-based ConservativeGenotypeQuality (GQX) value for one sample, reflecting the conservative confidence of the called genotype.

    \item \texttt{NormalizedAltHaplotypeCountRatio} ~ For variants in an active region, the proportion of reads supporting the top 2 haplotypes, or 0 if haplotyping failed due to this proportion being below threshold. For heterozygous variants with only one non-reference allele, the proportion is doubled so that its value is expected to be close to 1.0 regardless of genotype. The feature is set to -1 for variants not in an active region.

    \item \texttt{SampleIndelRepeatCount} ~ The number of times the primary indel allele's \emph{repeat unit} occurs in a haplotype containing the indel allele. The primary indel allele's \emph{repeat unit} is the smallest possible sequence such that the inserted/deleted sequence can be formed by concatenating multiple copies of it. The primary indel allele is the best supported allele among all overlapping indel alleles at the locus of interest in one sample.

    \item \texttt{SampleIndelRepeatUnitSize} ~ Length of the primary indel allele's \emph{repeat unit}, as defined for feature \texttt{SampleIndelRepeatCount}.

    \item \texttt{SampleIndelAlleleBiasLower} ~ For one sample, this is the negative log probability of seeing N or fewer observations of one allele in a heterozygous variant out of the total observations from both alleles. N is typically the observation count of the reference allele. If the heterozygous variant does not include the reference allele, the first indel allele is used instead.

    \item \texttt{SampleIndelAlleleBias} ~ Similar to \texttt{SampleIndelAlleleBiasLower}, except the count used is twice the count of the least frequently observed allele.

    \item \texttt{SampleProxyRMSMappingQuality} ~ RMS mapping quality of all reads spanning the position immediately proceeding the indel in one sample. This feature approximates the SAMPLE/MQ value defined in the VCF spec.

    \item \texttt{SamplePrimaryAltAlleleDepthFraction} ~ For one sample, this is the ratio of the confident observation count of the best-supported non-reference allele at the variant locus, over all confident allele observation counts.

    \item \texttt{ContextCompressability} ~ The length of the upstream or downstream reference context (whichever is greater) that can be represented using 5 Ziv-Lempel keywords \cite{lesne2009,ziv1977}. The Ziv-Lempel keywords are obtained using the scheme of Ziv and Lempel 1977 \cite{ziv1977}, by traversing the sequence and successively selecting the shortest subsequence that has not yet been encountered.

    \item \texttt{IndelCategory} ~ A binary variable set to 1 if the indel allele is a primitive deletion or 0 otherwise.

    \item \texttt{SamplePrimaryAltAlleleDepth} ~ The confident observation count of the best-supported non-reference allele at the variant locus.

    \item \texttt{VariantAlleleQuality} ~ The model-based variant quality value reflecting confidence that the called variant is present in at least one sample, regardless of genotype. This feature matches QUAL in the VCF spec

    \item \texttt{SampleMeanDistanceFromReadEdge} ~ For all non-reference basecall observations in one sample at a candidate SNV site, report the mean distance to the closest edge of each alternate basecall's read. Distance is measured in read-coordinates, zero-indexed, and is allowed to have a maximum value of 20.

    \item \texttt{SampleRefAlleleDepth} ~ The confident observation count of the reference allele at the variant locus.

    \item \texttt{SampleIndelMeanDistanceFromReadEdge} ~ For all indel allele observations in one sample at a candidate indel locus, report the mean distance to the closest edge of each indel allele's read. Distance is measured in read-coordinates, zero-indexed, and is allowed to have a maximum value of 20. The left or right side of the indel may be used to provide the shortest distance, but the indel will only be considered in its left-aligned position.

    \item \texttt{SampleRefRepeatCount} ~ The number of times the primary indel allele's \emph{repeat unit} occurs in the reference sequence.

\end{itemize}


\subsubsection{Somatic features}

Note that for somatic features "all samples" refers to the tumor and matched normal samples together.

\begin{itemize}

    \item \texttt{SomaticSNVQualityGivenGermlineGenotype} ~ Posterior probability of a somatic SNV conditioned on given germline genotype. This feature matches INFO/QSS\_NT in the VCF output.

    \item \texttt{NormalSampleRelativeTotalLocusDepth} ~ This feature matches the germline \texttt{RelativeTotalLocusDepth} feature, except that it reflects the depth of only the matched normal sample.

    \item \texttt{TumorSampleAltAlleleFraction} ~ Fraction of the tumor sample's tier1 observations which are not the reference allele (see: ``\nameref{sec:somatic_tiers}''). This is restricted to a maximum of 0.5 to prevent the model from overtraining against high somatic allele frequencies (these might be common e.g. for loss of heterozygosity regions from liquid tumors).

    \item \texttt{RMSMappingQuality} ~ Root mean square read mapping quality of all reads spanning the variant in all samples. This feature matches INFO/MQ in the VCF spec.

    \item \texttt{ZeroMappingQualityFraction} ~ Fraction of read mapping qualities equal to zero, for all reads spanning the variant in all samples.

    \item \texttt{TumorSampleStrandBias} ~ Log ratio of the tumor-sample somatic allele likelihood computed assuming the somatic allele occurs on only one strand vs both strands
    (thus higher values indicate greater bias).

    \item \texttt{TumorSampleReadPosRankSum} ~ Z-score of Mann-Whitney U test for reference vs non-reference allele read positions in the tumor sample's tier1 observations (see: ``\nameref{sec:somatic_tiers}'').

    \item \texttt{AlleleCountLogOddsRatio} ~ The log odds ratio of allele counts $log{\frac{r_t a_n} {r_n a_t}}$, given reference $(r_t,r_n)$ and non-reference $(a_t,a_n)$ allele counts for the tumor and normal sample pair.

    \item \texttt{NormalSampleFilteredDepthFraction} ~ The fraction of reads that were filtered out of the normal sample before calling the variant locus.

    \item \texttt{TumorSampleFilteredDepthFraction} ~ The fraction of reads that were filtered out of the tumor sample before calling the variant locus.

    \item \texttt{SomaticIndelQualityGivenGermlineGenotype} ~ Posterior probability of a somatic indel conditioned on given germline genotype. This feature matches INFO/QSI\_NT in the VCF output.

    \item \texttt{TumorSampleLogSymmetricStrandOddsRatio} ~ Log of the symmetric strand odds ratio of allele counts $log{\left( \frac{r_{fwd} a_{rev}} {r_{rev} a_{fwd}} + \frac{r_{rev} a_{fwd}} {r_{fwd} a_{rev}}\right)}$, given reference $(r_{fwd},r_{rev})$ and non-reference $(a_{fwd},a_{rev})$ confident counts of the tumor sample's tier1 observations (see: ``\nameref{sec:somatic_tiers}'').

    \item \texttt{RepeatUnitLength} ~ The length of the somatic indel allele's \emph{repeat unit}. The \emph{repeat unit} is the smallest possible sequence such that the inserted/deleted sequence can be formed by concatenating multiple copies of it.

    \item \texttt{IndelRepeatCount} ~ The number of times the somatic indel allele's \emph{repeat unit} occurs in a haplotype containing the indel allele.

    \item \texttt{RefRepeatCount} ~ The number of times the somatic indel allele's \emph{repeat unit} occurs in the reference sequence.

    \item \texttt{TumorSampleIndelNoiseLogOdds} ~ Log ratio of the frequency of the candidate indel vs all other indels at the same locus in the tumor sample. The frequencies are computed from reads which confidently support a single allele at the locus, and only include reads which qualify as tier1 observations in the tumor sample (see: ``\nameref{sec:somatic_tiers}'').

    \item \texttt{TumorNormalIndelAlleleLogOdds} ~ Log ratio of the frequency of the candidate indel in the tumor vs normal samples. The frequencies are computed from reads which confidently support a single allele at the locus, and only include reads which qualify as tier1 observations in each sample (see: ``\nameref{sec:somatic_tiers}'').

    \item \texttt{SiteFilteredBasecallFrac} ~ The maximum value over all samples of \texttt{SampleSiteFilteredBasecallFrac}, which is the fraction of basecalls at a site which have been removed by the mismatch density filter in a given sample.

    \item \texttt{IndelWindowFilteredBasecallFrac} ~ The maximum value over all samples of \texttt{SampleIndelWindowFilteredBasecallFrac}, which is the fraction of basecalls in a window extending 50 bases to each side of the candidate indel's call position which have been removed by the mismatch density filter in a given sample.

    \item \texttt{SpanningDeletionFraction} ~ The maximum value over all samples of \texttt{SampleSpanningDeletionFraction}, which is the fraction of reads crossing a candidate SNV site with spanning deletions in a given sample.
\end{itemize}

\bibliographystyle{alpha}
\bibliography{methods}

\end{document}
